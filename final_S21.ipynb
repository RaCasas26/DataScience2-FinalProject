{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final_S21.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RaCasas26/DataScience2-FinalProject/blob/main/final_S21.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNlpEYHNjcT_"
      },
      "source": [
        "#  Final Project\n",
        "## Data Science (masters)\n",
        "## Math 5364 & 5366, Fall 20 & Spring 21\n",
        "## Tarleton State University\n",
        "## Dr. Scott Cook\n",
        "## Due 2021-05-14"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05beGOpEJggA"
      },
      "source": [
        "## Background\n",
        "\n",
        "In North America, Monarch butterflies (*Danaus plexippus*) are one of the many species of insects that migrate during the winter. However, Monarch butterflies that inhabit other regions such as the Pacific or South America are known to not migrate. A study by Freedman et al. (2020) considers the differences in physical traits between nonmigratory and migratory Monarch butterflies. Specifically, the reserachers of this study compared the differences betweens in the forewing length between Monarch butterflies in North America, the Pacific, the Atlantic, South America, and Australia. By conducting a time series analyses on data collected since 1856, the researchers determined that not only did migratory Monarch butterflies have larger forewings than their nonmigratory counterparts but that the wing size of nonmigratory butterflies reduced over a period of 1000 years.\n",
        "\n",
        "The dataset appied in this study that considered several variables pertaining to the physical characteristics of different Monarch butterfly samples caught in the wild and in captivity throughout North America, South America, and Australia as well as throughout the Pacific and Atlantic regions. To explore potential models from this dataset, classifiers and regressors were considered in predicting the area a Monarch butterfly comes from as well as in predicting the era the butterfly was caught based off of the physical traits given in this study."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXCBNKYzUARp"
      },
      "source": [
        "## Data Processing and Exploration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrV6I7kE-HFz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fd42b2d-23c6-446b-9a95-6118132e2781"
      },
      "source": [
        "! pip install --upgrade numpy\n",
        "! pip install --upgrade pandas"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: numpy in /usr/local/lib/python3.7/dist-packages (1.20.3)\n",
            "Requirement already up-to-date: pandas in /usr/local/lib/python3.7/dist-packages (1.2.4)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.20.3)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5MUvP7dpJzC",
        "outputId": "cf746818-e13b-41bb-c255-cb4826e773d9"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "import io\n",
        "from copy import deepcopy  \n",
        "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.svm import LinearSVC, LinearSVR\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler \n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import f1_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "pd.set_option('display.max_columns', 25)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFoQJikA-55f"
      },
      "source": [
        "# Find the wings_04.25.20.csv file in the Files menu on the left\n",
        "# Right click the file and click Copy Path\n",
        "# Paste the path between the quotation marks below\n",
        "path = \"/content/drive/MyDrive/Data_ScienceS21_Final/wings_04.25.20.csv\"\n",
        "df = pd.read_csv(path)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SybTFd3p0lHi"
      },
      "source": [
        "### Dataset\n",
        "The dataset has 7039 observations with 27 variables. Of those variables, the ones of interest were the region and country the butterfly was collected, the sex of the butterfly, and the butterfly's forewing measurements. As a result, only 14 variables were considered in the making of the models. \\\\\n",
        "Many values were missing in this dataset so many observations had to be removed or imputed. Since butterfly wings are symmetric, measurements for the left forewing were replaced with the measurements of the right forewing if any of those values were missing and vice versa if measurements of the right forewing were missing. If both measurements were missing, the observation was dropped from the dataset. Cleaning this dataset left a total of 6589 observations to be used for analysis. \\\\\n",
        "The study by Freedman et al. (2020) applied PCA to the forewing length, width, and area to create the size principal componenent which accounted for 96.4% of the forewing size variation and the study applied PCA to the forewing aspect ratio and roundness to create the shape principal component which accounted for 86% of the forewing shape variation. These principal components were created in a similar fashion in order to be applied to the analysis. \n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZf2aqA_YGZB"
      },
      "source": [
        "df_orig = df.copy()\n",
        "\n",
        "#Removes variables not involved in analysis\n",
        "df = df.drop(['Index','SampleID','Collection', 'Island/State', 'County/District', 'Site/City', 'exact_location?',\n",
        "       'lon', 'Wild-Caught?', 'image_type', 'smoothing', 'Host_plant','observer'], axis=1)\n",
        "\n",
        "#Converts date into year from its original format (YYYYDDMM)\n",
        "df['Collection_Year'] = (df['Collection_Date'] * 10 ** (-4)).round(0)\n",
        "#Removes Collection Date which will not be used in the analysis\n",
        "df = df.drop('Collection_Date', axis=1)\n",
        "\n",
        "#Removes the gyandromorph sex so that only male and female monarch butterlies are considered for the analysis\n",
        "df = df[df['Sex'] != \"gyandromorph\"]\n",
        "\n",
        "#Removes observations which had no location of capture \n",
        "df = df[pd.notna(df['Region'])]\n",
        "#Removes observations in the Indian Ocean which are too few for analysis\n",
        "df = df[df['Region'] != 'Indian_Ocean']\n",
        "\n",
        "#Binarizes the sexes\n",
        "df = df.replace(to_replace={'male': 0, 'female': 1})"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evRW46Ohz3zx"
      },
      "source": [
        "vals = ['Length', 'Width', 'Area', 'Perimeter']\n",
        "for v in vals:\n",
        "    cols = [f'{s}{v}' for s in ['L', 'R']]\n",
        "    f = np.nanmean(df[cols], axis=1)\n",
        "    # print(f)\n",
        "    for c in cols:\n",
        "        mask = df[c].isnull()\n",
        "        df.loc[mask, c] = f[mask]\n",
        "    df.dropna(subset=cols, inplace=True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vi1TCZ3z-mvx"
      },
      "source": [
        "#Calculates Aspect Ration and Roundness of left and right forewings, respectively\n",
        "df['LAspect_Ratio'] = df['LLength']/df['LWidth']\n",
        "df['RAspect_Ratio'] = df['RLength']/df['RWidth']\n",
        "df['LRoundness'] = 4*np.pi*df['LArea'] / df['LPerimeter'] ** 2\n",
        "df['RRoundness'] = 4*np.pi*df['RArea'] / df['RPerimeter'] ** 2"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSLg7mY7f328"
      },
      "source": [
        "#Binarizes the 'overwintering?' variable\n",
        "df = df.replace(to_replace={'no': 0, 'yes': 1})"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcYrmawOBmN7"
      },
      "source": [
        "df = pd.DataFrame(df.to_numpy(), columns=['Region', 'Country/Archipelago', 'lat', 'overwintering?', 'Sex',\n",
        "       'LLength', 'LWidth', 'LArea', 'LPerimeter', 'RLength', 'RWidth',\n",
        "       'RArea', 'RPerimeter', 'Collection_Year', 'LAspect_Ratio',\n",
        "       'RAspect_Ratio', 'LRoundness', 'RRoundness'])\n",
        "df[['lat', 'overwintering?', 'Sex','LLength', 'LWidth', 'LArea', 'LPerimeter', 'RLength', 'RWidth',\n",
        "       'RArea', 'RPerimeter', 'Collection_Year', 'LAspect_Ratio',\n",
        "       'RAspect_Ratio', 'LRoundness', 'RRoundness']] = df[['lat', 'overwintering?', 'Sex','LLength', 'LWidth', 'LArea', 'LPerimeter', 'RLength', 'RWidth',\n",
        "       'RArea', 'RPerimeter', 'Collection_Year', 'LAspect_Ratio',\n",
        "       'RAspect_Ratio', 'LRoundness', 'RRoundness']].astype('float64')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RG7LaelOLwM1"
      },
      "source": [
        "#Applies PCA to compute the first components describing size and shape as described in Freedman et al. (2020)\n",
        "SizePC1 = pd.DataFrame(PCA(n_components=1, random_state=42).fit_transform(df[['LLength', 'LWidth', 'LArea', 'RLength', 'RWidth', 'RArea']]), columns= ['SizePC1'])\n",
        "ShapePC1 = pd.DataFrame(PCA(n_components=1, random_state=42).fit_transform(df[['LAspect_Ratio','RAspect_Ratio', 'LRoundness', 'RRoundness']]), columns= ['ShapePC1'])\n",
        "df = pd.concat([df.reindex(index=ShapePC1.index), ShapePC1, SizePC1], axis=1)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "hTzaKuYai81O",
        "outputId": "b8801ca7-500e-41b9-a705-9da87ccab55b"
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lat</th>\n",
              "      <th>overwintering?</th>\n",
              "      <th>Sex</th>\n",
              "      <th>LLength</th>\n",
              "      <th>LWidth</th>\n",
              "      <th>LArea</th>\n",
              "      <th>LPerimeter</th>\n",
              "      <th>RLength</th>\n",
              "      <th>RWidth</th>\n",
              "      <th>RArea</th>\n",
              "      <th>RPerimeter</th>\n",
              "      <th>Collection_Year</th>\n",
              "      <th>LAspect_Ratio</th>\n",
              "      <th>RAspect_Ratio</th>\n",
              "      <th>LRoundness</th>\n",
              "      <th>RRoundness</th>\n",
              "      <th>ShapePC1</th>\n",
              "      <th>SizePC1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>6560.000000</td>\n",
              "      <td>6589.000000</td>\n",
              "      <td>6589.000000</td>\n",
              "      <td>6589.000000</td>\n",
              "      <td>6589.000000</td>\n",
              "      <td>6589.000000</td>\n",
              "      <td>6589.000000</td>\n",
              "      <td>6589.000000</td>\n",
              "      <td>6589.00000</td>\n",
              "      <td>6589.000000</td>\n",
              "      <td>6589.000000</td>\n",
              "      <td>6123.000000</td>\n",
              "      <td>6589.000000</td>\n",
              "      <td>6589.000000</td>\n",
              "      <td>6589.000000</td>\n",
              "      <td>6589.000000</td>\n",
              "      <td>6.589000e+03</td>\n",
              "      <td>6.589000e+03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>13.310120</td>\n",
              "      <td>0.044316</td>\n",
              "      <td>0.444377</td>\n",
              "      <td>4.911631</td>\n",
              "      <td>2.506502</td>\n",
              "      <td>8.004036</td>\n",
              "      <td>11.779823</td>\n",
              "      <td>4.926687</td>\n",
              "      <td>2.50809</td>\n",
              "      <td>8.042228</td>\n",
              "      <td>11.820600</td>\n",
              "      <td>1961.449943</td>\n",
              "      <td>1.960393</td>\n",
              "      <td>1.965098</td>\n",
              "      <td>0.722211</td>\n",
              "      <td>0.720750</td>\n",
              "      <td>-8.077930e-17</td>\n",
              "      <td>9.107906e-16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>25.139305</td>\n",
              "      <td>0.205812</td>\n",
              "      <td>0.496934</td>\n",
              "      <td>0.356998</td>\n",
              "      <td>0.177482</td>\n",
              "      <td>1.061390</td>\n",
              "      <td>0.850554</td>\n",
              "      <td>0.359470</td>\n",
              "      <td>0.17692</td>\n",
              "      <td>1.061425</td>\n",
              "      <td>0.853559</td>\n",
              "      <td>34.949535</td>\n",
              "      <td>0.066570</td>\n",
              "      <td>0.067042</td>\n",
              "      <td>0.020638</td>\n",
              "      <td>0.021711</td>\n",
              "      <td>9.180988e-02</td>\n",
              "      <td>1.589064e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-40.857807</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.211000</td>\n",
              "      <td>1.625000</td>\n",
              "      <td>3.542000</td>\n",
              "      <td>7.731000</td>\n",
              "      <td>2.935000</td>\n",
              "      <td>1.67000</td>\n",
              "      <td>3.606000</td>\n",
              "      <td>7.806000</td>\n",
              "      <td>1856.000000</td>\n",
              "      <td>1.546780</td>\n",
              "      <td>1.000341</td>\n",
              "      <td>0.570151</td>\n",
              "      <td>0.583013</td>\n",
              "      <td>-6.625055e-01</td>\n",
              "      <td>-6.088675e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>-6.801374</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.709000</td>\n",
              "      <td>2.407000</td>\n",
              "      <td>7.386000</td>\n",
              "      <td>11.299000</td>\n",
              "      <td>4.722000</td>\n",
              "      <td>2.41100</td>\n",
              "      <td>7.421000</td>\n",
              "      <td>11.338000</td>\n",
              "      <td>1935.000000</td>\n",
              "      <td>1.919387</td>\n",
              "      <td>1.922643</td>\n",
              "      <td>0.709687</td>\n",
              "      <td>0.708001</td>\n",
              "      <td>-5.638972e-02</td>\n",
              "      <td>-1.080165e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>18.109580</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.951000</td>\n",
              "      <td>2.525000</td>\n",
              "      <td>8.087000</td>\n",
              "      <td>11.870000</td>\n",
              "      <td>4.972000</td>\n",
              "      <td>2.52500</td>\n",
              "      <td>8.130000</td>\n",
              "      <td>11.922000</td>\n",
              "      <td>1965.000000</td>\n",
              "      <td>1.959316</td>\n",
              "      <td>1.964615</td>\n",
              "      <td>0.722689</td>\n",
              "      <td>0.721771</td>\n",
              "      <td>-8.371775e-04</td>\n",
              "      <td>-1.364584e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>35.315741</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5.158000</td>\n",
              "      <td>2.626000</td>\n",
              "      <td>8.725000</td>\n",
              "      <td>12.371000</td>\n",
              "      <td>5.178000</td>\n",
              "      <td>2.63000</td>\n",
              "      <td>8.771000</td>\n",
              "      <td>12.419000</td>\n",
              "      <td>1984.000000</td>\n",
              "      <td>2.000390</td>\n",
              "      <td>2.005702</td>\n",
              "      <td>0.735385</td>\n",
              "      <td>0.734510</td>\n",
              "      <td>5.422625e-02</td>\n",
              "      <td>9.176677e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>52.920633</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>6.033000</td>\n",
              "      <td>3.219000</td>\n",
              "      <td>12.184000</td>\n",
              "      <td>14.685000</td>\n",
              "      <td>6.011000</td>\n",
              "      <td>3.21500</td>\n",
              "      <td>12.110000</td>\n",
              "      <td>14.653000</td>\n",
              "      <td>2018.000000</td>\n",
              "      <td>2.803137</td>\n",
              "      <td>2.484677</td>\n",
              "      <td>0.801186</td>\n",
              "      <td>0.800982</td>\n",
              "      <td>9.340516e-01</td>\n",
              "      <td>6.841371e+00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               lat  overwintering?          Sex      LLength       LWidth  \\\n",
              "count  6560.000000     6589.000000  6589.000000  6589.000000  6589.000000   \n",
              "mean     13.310120        0.044316     0.444377     4.911631     2.506502   \n",
              "std      25.139305        0.205812     0.496934     0.356998     0.177482   \n",
              "min     -40.857807        0.000000     0.000000     3.211000     1.625000   \n",
              "25%      -6.801374        0.000000     0.000000     4.709000     2.407000   \n",
              "50%      18.109580        0.000000     0.000000     4.951000     2.525000   \n",
              "75%      35.315741        0.000000     1.000000     5.158000     2.626000   \n",
              "max      52.920633        1.000000     1.000000     6.033000     3.219000   \n",
              "\n",
              "             LArea   LPerimeter      RLength      RWidth        RArea  \\\n",
              "count  6589.000000  6589.000000  6589.000000  6589.00000  6589.000000   \n",
              "mean      8.004036    11.779823     4.926687     2.50809     8.042228   \n",
              "std       1.061390     0.850554     0.359470     0.17692     1.061425   \n",
              "min       3.542000     7.731000     2.935000     1.67000     3.606000   \n",
              "25%       7.386000    11.299000     4.722000     2.41100     7.421000   \n",
              "50%       8.087000    11.870000     4.972000     2.52500     8.130000   \n",
              "75%       8.725000    12.371000     5.178000     2.63000     8.771000   \n",
              "max      12.184000    14.685000     6.011000     3.21500    12.110000   \n",
              "\n",
              "        RPerimeter  Collection_Year  LAspect_Ratio  RAspect_Ratio  \\\n",
              "count  6589.000000      6123.000000    6589.000000    6589.000000   \n",
              "mean     11.820600      1961.449943       1.960393       1.965098   \n",
              "std       0.853559        34.949535       0.066570       0.067042   \n",
              "min       7.806000      1856.000000       1.546780       1.000341   \n",
              "25%      11.338000      1935.000000       1.919387       1.922643   \n",
              "50%      11.922000      1965.000000       1.959316       1.964615   \n",
              "75%      12.419000      1984.000000       2.000390       2.005702   \n",
              "max      14.653000      2018.000000       2.803137       2.484677   \n",
              "\n",
              "        LRoundness   RRoundness      ShapePC1       SizePC1  \n",
              "count  6589.000000  6589.000000  6.589000e+03  6.589000e+03  \n",
              "mean      0.722211     0.720750 -8.077930e-17  9.107906e-16  \n",
              "std       0.020638     0.021711  9.180988e-02  1.589064e+00  \n",
              "min       0.570151     0.583013 -6.625055e-01 -6.088675e+00  \n",
              "25%       0.709687     0.708001 -5.638972e-02 -1.080165e+00  \n",
              "50%       0.722689     0.721771 -8.371775e-04 -1.364584e-01  \n",
              "75%       0.735385     0.734510  5.422625e-02  9.176677e-01  \n",
              "max       0.801186     0.800982  9.340516e-01  6.841371e+00  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5Ju-H7OsFHb"
      },
      "source": [
        "Several different questions were considered in the analysis of the dataset:\n",
        "    \n",
        "    1. Could physical traits and sex help determine the region the Monarch butterfly was collected?\n",
        "    2. Could physical traits and sex help determine the country the Monarch butterfly was collected?\n",
        "    3. Could physical traits and sex help determine where in North America migratory butterflies were originally from? \n",
        "    4. Could physical traits, sex, and region help determine what year the butterfly was collected? \n",
        "\n",
        "To answer each of these questions, the dataframes were formated to exclude missing values from the target variables and to exclude classes with fewer that 5 observations to aviod issues with the algorithms. All models included the shape and size principal components and the sex of the Monarch butterfly as the predicting variables. Only the model for question 4 inlcuded the region as a predicting variable. \n",
        "The dataset for each model was split into modeling and holdouts sets in which 80% of the data was allocated into the modeling set and 20% of the data was allocated into the holdout set using a stratified shuffle split algorithm for the classifiers and a shuffle split algorithm for the regressor. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQfm3dvL20qK"
      },
      "source": [
        "#Modelling datasets for predicting region based on physical traits\n",
        "df_reg = df[pd.notna(df['Region'])]\n",
        "X_reg = df_reg[['ShapePC1', 'SizePC1' ,'Sex']]\n",
        "y_reg = df_reg['Region']\n",
        "\n",
        "holdout_frac = 0.20\n",
        "holdout_splitter = StratifiedShuffleSplit(test_size=holdout_frac, random_state=42)\n",
        "model_idx, holdout_idx = next(holdout_splitter.split(X_reg, y_reg))\n",
        "X_reg_m, y_reg_m = X_reg.iloc[model_idx]  , y_reg.iloc[model_idx]\n",
        "X_reg_h, y_reg_h = X_reg.iloc[holdout_idx], y_reg.iloc[holdout_idx]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDY7vqH-4vTX"
      },
      "source": [
        "#Modelling datasets for predicting country based on physical traits\n",
        "df_country = df[pd.notna(df['Country/Archipelago'])]\n",
        "df_country = df_country[(df['Country/Archipelago'] != 'Tokelau') & \n",
        "                (df['Country/Archipelago'] != 'Cape_Verde') &\n",
        "                (df['Country/Archipelago'] != 'Philippines') &\n",
        "                (df['Country/Archipelago'] != 'Argentina') &\n",
        "                (df['Country/Archipelago'] != 'Saint_Lucia') &\n",
        "                (df['Country/Archipelago'] != 'Kiribati') &\n",
        "                (df['Country/Archipelago'] != 'Palau') &\n",
        "                (df['Country/Archipelago'] != 'Kermedec_Islands') &\n",
        "                (df['Country/Archipelago'] != 'Tobago') &\n",
        "                (df['Country/Archipelago'] != 'Marshall_Islands') &\n",
        "                (df['Country/Archipelago'] != 'French_Guyana') &\n",
        "                (df['Country/Archipelago'] != 'Belize') &\n",
        "                (df['Country/Archipelago'] != 'Azores') &\n",
        "                (df['Country/Archipelago'] != 'Cook_Islands') &\n",
        "                (df['Country/Archipelago'] != 'Bermuda') &\n",
        "                (df['Country/Archipelago'] != 'Taiwan') &\n",
        "                (df['Country/Archipelago'] != 'Marquesas')]\n",
        "#group by, filter\n",
        "X_ctry = df_country[['ShapePC1', 'SizePC1' ,'Sex']]\n",
        "y_ctry = df_country['Country/Archipelago']\n",
        "\n",
        "holdout_frac = 0.20\n",
        "holdout_splitter = StratifiedShuffleSplit(n_splits = 5, test_size=holdout_frac, random_state=42)\n",
        "model_idx, holdout_idx = next(holdout_splitter.split(X_ctry, y_ctry))\n",
        "X_ctry_m, y_ctry_m = X_ctry.iloc[model_idx]  , y_ctry.iloc[model_idx]\n",
        "X_ctry_h, y_ctry_h = X_ctry.iloc[holdout_idx], y_ctry.iloc[holdout_idx]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8817Pox47ye"
      },
      "source": [
        "#Modelling datasets for predicting the origin of migratory butterflies in North America based on physical traits\n",
        "df_lat = df[pd.notna(df['lat'])]\n",
        "df_lat = df_lat[(df_lat['Region'] == 'North_America') & (df_lat['overwintering?'] == 0)]\n",
        "\n",
        "X_lat = df_lat[['ShapePC1', 'SizePC1' ,'Sex']]\n",
        "y_lat = df_lat['lat']\n",
        "\n",
        "holdout_frac = 0.20\n",
        "holdout_splitter = ShuffleSplit(test_size=holdout_frac, random_state=42)\n",
        "model_idx, holdout_idx = next(holdout_splitter.split(X_lat, y_lat))\n",
        "X_lat_m, y_lat_m = X_lat.iloc[model_idx]  , y_lat.iloc[model_idx]\n",
        "X_lat_h, y_lat_h = X_lat.iloc[holdout_idx], y_lat.iloc[holdout_idx]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JI8LSqMl5IrU"
      },
      "source": [
        "#Modelling datasets for predicting year based on physical traits and region\n",
        "df_yr = df[pd.notna(df['Collection_Year'])]\n",
        "df_yr = pd.concat([df_yr, pd.get_dummies(df_yr['Region']).astype('float64')], axis = 1).reindex(df_yr.index)\n",
        "\n",
        "X_yr = df_yr[['ShapePC1', 'SizePC1' ,'Sex', 'Atlantic', 'Caribbean', 'Central_America', 'North_America',\n",
        "       'Pacific_Islands', 'South_America']]\n",
        "y_yr = df_yr['Collection_Year']\n",
        "\n",
        "holdout_frac = 0.20\n",
        "holdout_splitter = ShuffleSplit(test_size=holdout_frac, random_state=42)\n",
        "model_idx, holdout_idx = next(holdout_splitter.split(X_yr, y_yr))\n",
        "X_yr_m, y_yr_m = X_yr.iloc[model_idx]  , y_yr.iloc[model_idx]\n",
        "X_yr_h, y_yr_h = X_yr.iloc[holdout_idx], y_yr.iloc[holdout_idx]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaW-Spx01sUa"
      },
      "source": [
        "## Supervised Model Building and Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgilowL7seOr"
      },
      "source": [
        "The classifiers applied were the K-Nearest Neighbors classifier, the Random Forest classifier, and the Linear Support Vector Machine classifier. For all models, three different scalers were used to transform the data. Those being the standard scaler, the min-max scaler, and the robust scalers.\n",
        " \n",
        "For the K-Nearest Neighbor classifier, the hyperparameters of interest were the number of neighbors and the weight function used for the prediction. The number of neighbors chosen were multiples of 5 up until 25 and the weight functions considered were the uniform and distance weight functions. \n",
        "\n",
        "For the Random Forest classifier, the hyperparameter of interest was the maximum depth of each decision tree. The maximum depth of each decision tree were multiples of 2 up until 10.\n",
        "\n",
        "For the Linear Support Vector Machine classifier, the hyperparameter of interest was the regularization parameter, $C$. The parameter $C$ was given by 5 equally spaced points from 0.5 to 3.\n",
        "\n",
        "For each model, k-fold cross validation was performed using the weighted $F1$-score as a performance metric."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lymaqqH-iUdE"
      },
      "source": [
        "class TopModel_Classifier:\n",
        "    def __init__(self, X_m, y_m):\n",
        "        self.X_m = X_m\n",
        "        self.y_m = y_m\n",
        "\n",
        "    def top_knc(self):\n",
        "        scaler_type = [StandardScaler, MinMaxScaler, RobustScaler]\n",
        "        grid_k = {}\n",
        "        for s in scaler_type:\n",
        "            pipe = Pipeline([('scale', s()),\n",
        "                            ('classify'  , KNeighborsClassifier())\n",
        "                            ])\n",
        "\n",
        "            hyperparams = {\n",
        "                        'classify__n_neighbors' : np.concatenate((np.arange(1,2), np.arange(5,26,5)), axis = None),\n",
        "                        'classify__weights' : ['uniform', 'distance']\n",
        "                        }\n",
        "            grid_k[\"grid_\" + s.__name__] = GridSearchCV(pipe, hyperparams, cv=10, scoring='f1_weighted').fit(self.X_m, self.y_m)\n",
        "        res1 = grid_k[\"grid_StandardScaler\"].cv_results_\n",
        "        df1 = pd.DataFrame(res1['params'])\n",
        "        df1['score'] = res1['mean_test_score']\n",
        "        df1['scale'] = 'standard'\n",
        "\n",
        "        res2 = grid_k[\"grid_MinMaxScaler\"].cv_results_\n",
        "        df2 = pd.DataFrame(res2['params'])\n",
        "        df2['score'] = res2['mean_test_score']\n",
        "        df2['scale'] = 'min_max'\n",
        "\n",
        "        res3 = grid_k[\"grid_RobustScaler\"].cv_results_\n",
        "        df3 = pd.DataFrame(res3['params'])\n",
        "        df3['score'] = res3['mean_test_score']\n",
        "        df3['scale'] = 'robust'\n",
        "\n",
        "        res = [df1, df2, df3]\n",
        "        df = pd.concat(res, ignore_index = True)\n",
        "        M = df['score'].quantile(q = 0.95)\n",
        "        mask = df['score'] > M\n",
        "        return df[mask].sort_values(by = 'score', ascending = False)\n",
        "\n",
        "    def top_rfc(self):\n",
        "        scaler_type = [StandardScaler, MinMaxScaler, RobustScaler]\n",
        "        grid_k = {}\n",
        "        for s in scaler_type:\n",
        "            pipe = Pipeline([('scale', s()),\n",
        "                            ('classify' , RandomForestClassifier(random_state=42))\n",
        "                            ])\n",
        "\n",
        "            hyperparams = {\n",
        "                        'classify__max_depth' : np.arange(2, 11, 2)\n",
        "                        }\n",
        "            grid_k[\"grid_\" + s.__name__] = GridSearchCV(pipe, hyperparams, cv=10, scoring='f1_weighted').fit(self.X_m, self.y_m)\n",
        "        res1 = grid_k[\"grid_StandardScaler\"].cv_results_\n",
        "        df1 = pd.DataFrame(res1['params'])\n",
        "        df1['score'] = res1['mean_test_score']\n",
        "        df1['scale'] = 'standard'\n",
        "\n",
        "        res2 = grid_k[\"grid_MinMaxScaler\"].cv_results_\n",
        "        df2 = pd.DataFrame(res2['params'])\n",
        "        df2['score'] = res2['mean_test_score']\n",
        "        df2['scale'] = 'min_max'\n",
        "\n",
        "\n",
        "        res3 = grid_k[\"grid_RobustScaler\"].cv_results_\n",
        "        df3 = pd.DataFrame(res3['params'])\n",
        "        df3['score'] = res3['mean_test_score']\n",
        "        df3['scale'] = 'robust'\n",
        "\n",
        "        res = [df1, df2, df3]\n",
        "        df = pd.concat(res, ignore_index = True)\n",
        "\n",
        "        M = df['score'].quantile(q = 0.95)\n",
        "        mask = df['score'] > M\n",
        "        return df[mask].sort_values(by = 'score', ascending = False)\n",
        "\n",
        "    def top_lsvc(self):\n",
        "        scaler_type = [StandardScaler, MinMaxScaler, RobustScaler]\n",
        "        grid_k = {}\n",
        "        for s in scaler_type:\n",
        "            pipe = Pipeline([('scale', s()),\n",
        "                            ('classify' , LinearSVC(random_state=42))\n",
        "                            ])\n",
        "\n",
        "            hyperparams = {\n",
        "                            'classify__C': np.arange(0.5, 3.5,0.5)\n",
        "                        }\n",
        "            grid_k[\"grid_\" + s.__name__] = GridSearchCV(pipe, hyperparams, cv=10, scoring='f1_weighted').fit(self.X_m, self.y_m)\n",
        "        res1 = grid_k[\"grid_StandardScaler\"].cv_results_\n",
        "        df1 = pd.DataFrame(res1['params'])\n",
        "        df1['score'] = res1['mean_test_score']\n",
        "        df1['scale'] = 'standard'\n",
        "\n",
        "        res2 = grid_k[\"grid_MinMaxScaler\"].cv_results_\n",
        "        df2 = pd.DataFrame(res2['params'])\n",
        "        df2['score'] = res2['mean_test_score']\n",
        "        df2['scale'] = 'min_max'\n",
        "\n",
        "\n",
        "        res3 = grid_k[\"grid_RobustScaler\"].cv_results_\n",
        "        df3 = pd.DataFrame(res3['params'])\n",
        "        df3['score'] = res3['mean_test_score']\n",
        "        df3['scale'] = 'robust'\n",
        "\n",
        "        res = [df1, df2, df3]\n",
        "        df = pd.concat(res, ignore_index = True)\n",
        "\n",
        "        M = df['score'].quantile(q = 0.95)\n",
        "        mask = df['score'] > M\n",
        "        return df[mask].sort_values(by = 'score', ascending = False)\n",
        "\n",
        "    def best_model(self):\n",
        "        knc_model = self.top_knc()\n",
        "        knc_model['model'] = 'knc'\n",
        "        rfc_model = self.top_rfc()\n",
        "        rfc_model['model'] = 'rfc'\n",
        "        lsvc_model = self.top_lsvc()\n",
        "        lsvc_model['model'] = 'lsvc'\n",
        "\n",
        "        mask1 = knc_model['score'] == knc_model['score'].max()\n",
        "        mask2 = rfc_model['score'] == rfc_model['score'].max()\n",
        "        mask3 = lsvc_model['score'] == lsvc_model['score'].max()\n",
        "\n",
        "        full_model = [knc_model[mask1], rfc_model[mask2], lsvc_model[mask3]]\n",
        "        first_highest_score = [knc_model[mask1]['score'].iloc[0], rfc_model[mask2]['score'].iloc[0], lsvc_model[mask3]['score'].iloc[0]]\n",
        "        return full_model[np.argmax(first_highest_score)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "K8wXGdTA_5v9",
        "outputId": "9a1181a4-f1fe-4868-8152-1cda4e369bf4"
      },
      "source": [
        "TopModel_Classifier(X_reg_m,y_reg_m).best_model()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>classify__max_depth</th>\n",
              "      <th>score</th>\n",
              "      <th>scale</th>\n",
              "      <th>model</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>8</td>\n",
              "      <td>0.437708</td>\n",
              "      <td>robust</td>\n",
              "      <td>rfc</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    classify__max_depth     score   scale model\n",
              "13                    8  0.437708  robust   rfc"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "YET-NNp5_5ew",
        "outputId": "6e515988-a7bc-4802-f470-bcd157118893"
      },
      "source": [
        "TopModel_Classifier(X_ctry_m,y_ctry_m).best_model()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>classify__n_neighbors</th>\n",
              "      <th>classify__weights</th>\n",
              "      <th>score</th>\n",
              "      <th>scale</th>\n",
              "      <th>model</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>20</td>\n",
              "      <td>uniform</td>\n",
              "      <td>0.225223</td>\n",
              "      <td>min_max</td>\n",
              "      <td>knc</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    classify__n_neighbors classify__weights     score    scale model\n",
              "20                     20           uniform  0.225223  min_max   knc"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6-Ar9Bi7yhw"
      },
      "source": [
        "The regressors applied were the K-Nearest Neighbors regressor, the Random Forest regressor, the Linear Support Vector Machine regressor, the LASSO regressor and the Ridge regressor. For all models, three different scalers were used to transform the data. Those being the standard scaler, the min-max scaler, and the robust scalers.\n",
        " \n",
        "For the K-Nearest Neighbor regressor, the hyperparameters of interest were the number of neighbors and the weight function used for the prediction. The number of neighbors chosen were multiples of 5 up until 25 and the weight functions considered were the uniform and distance weight functions. \n",
        "\n",
        "For the Random Forest regressor, the hyperparameter of interest was the maximum depth of each decision tree. The maximum depth of each decision tree were multiples of 2 up until 10.\n",
        "\n",
        "For the Linear Support Vector Machine regressor, the hyperparameter of interest was the regularization parameter, $C$. The parameter $C$ was given by 5 equally spaced points from 0.5 to 3.\n",
        "\n",
        "For the LASSO and Ridge regressors, the hyperparameter of interest was the coefficient penalty, $\\alpha$. The paramter $\\alpha$ was given by 9 equally spaced points from 0.2 to 2.\n",
        "\n",
        "For each model, k-fold cross validation was performed using $R^2$ as a performance metric."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pp5EpdXwxNyk"
      },
      "source": [
        "class TopModel_Regressor:\n",
        "    def __init__(self, X_m, y_m):\n",
        "        self.X_m = X_m\n",
        "        self.y_m = y_m\n",
        "\n",
        "    def top_knr(self):\n",
        "        scaler_type = [StandardScaler, MinMaxScaler, RobustScaler]\n",
        "        grid_k = {}\n",
        "        for s in scaler_type:\n",
        "            pipe = Pipeline([('scale', s()),\n",
        "                            ('classify'  , KNeighborsRegressor())\n",
        "                            ])\n",
        "\n",
        "            hyperparams = {\n",
        "                        'classify__n_neighbors' : np.concatenate((np.arange(1,2), np.arange(5,26,5)), axis = None),\n",
        "                        'classify__weights' : ['uniform', 'distance']\n",
        "                        }\n",
        "            grid_k[\"grid_\" + s.__name__] = GridSearchCV(pipe, hyperparams, cv=10, scoring='r2').fit(self.X_m, self.y_m)\n",
        "        res1 = grid_k[\"grid_StandardScaler\"].cv_results_\n",
        "        df1 = pd.DataFrame(res1['params'])\n",
        "        df1['score'] = res1['mean_test_score']\n",
        "        df1['scale'] = 'standard'\n",
        "\n",
        "        res2 = grid_k[\"grid_MinMaxScaler\"].cv_results_\n",
        "        df2 = pd.DataFrame(res2['params'])\n",
        "        df2['score'] = res2['mean_test_score']\n",
        "        df2['scale'] = 'min_max'\n",
        "\n",
        "        res3 = grid_k[\"grid_RobustScaler\"].cv_results_\n",
        "        df3 = pd.DataFrame(res3['params'])\n",
        "        df3['score'] = res3['mean_test_score']\n",
        "        df3['scale'] = 'robust'\n",
        "\n",
        "        res = [df1, df2, df3]\n",
        "        df = pd.concat(res, ignore_index = True)\n",
        "        M = df['score'].quantile(q = 0.95)\n",
        "        mask = df['score'] > M\n",
        "        return df[mask].sort_values(by = 'score', ascending = False)\n",
        "\n",
        "    def top_rfr(self):\n",
        "        scaler_type = [StandardScaler, MinMaxScaler, RobustScaler]\n",
        "        grid_k = {}\n",
        "        for s in scaler_type:\n",
        "            pipe = Pipeline([('scale', s()),\n",
        "                            ('classify' , RandomForestRegressor(random_state=42))\n",
        "                            ])\n",
        "\n",
        "            hyperparams = {\n",
        "                        'classify__max_depth' : np.arange(2, 11, 2)\n",
        "                        }\n",
        "            grid_k[\"grid_\" + s.__name__] = GridSearchCV(pipe, hyperparams, cv=10, scoring='r2').fit(self.X_m, self.y_m)\n",
        "        res1 = grid_k[\"grid_StandardScaler\"].cv_results_\n",
        "        df1 = pd.DataFrame(res1['params'])\n",
        "        df1['score'] = res1['mean_test_score']\n",
        "        df1['scale'] = 'standard'\n",
        "\n",
        "        res2 = grid_k[\"grid_MinMaxScaler\"].cv_results_\n",
        "        df2 = pd.DataFrame(res2['params'])\n",
        "        df2['score'] = res2['mean_test_score']\n",
        "        df2['scale'] = 'min_max'\n",
        "\n",
        "\n",
        "        res3 = grid_k[\"grid_RobustScaler\"].cv_results_\n",
        "        df3 = pd.DataFrame(res3['params'])\n",
        "        df3['score'] = res3['mean_test_score']\n",
        "        df3['scale'] = 'robust'\n",
        "\n",
        "        res = [df1, df2, df3]\n",
        "        df = pd.concat(res, ignore_index = True)\n",
        "\n",
        "        M = df['score'].quantile(q = 0.95)\n",
        "        mask = df['score'] > M\n",
        "        return df[mask].sort_values(by = 'score', ascending = False)\n",
        "\n",
        "    def top_lsvr(self):\n",
        "        scaler_type = [StandardScaler, MinMaxScaler, RobustScaler]\n",
        "        grid_k = {}\n",
        "        for s in scaler_type:\n",
        "            pipe = Pipeline([('scale', s()),\n",
        "                            ('classify' , LinearSVR(random_state=42))\n",
        "                            ])\n",
        "\n",
        "            hyperparams = {\n",
        "                            'classify__C': np.arange(0.5, 3.5,0.5)\n",
        "                        }\n",
        "            grid_k[\"grid_\" + s.__name__] = GridSearchCV(pipe, hyperparams, cv=10, scoring='r2').fit(self.X_m, self.y_m)\n",
        "        res1 = grid_k[\"grid_StandardScaler\"].cv_results_\n",
        "        df1 = pd.DataFrame(res1['params'])\n",
        "        df1['score'] = res1['mean_test_score']\n",
        "        df1['scale'] = 'standard'\n",
        "\n",
        "        res2 = grid_k[\"grid_MinMaxScaler\"].cv_results_\n",
        "        df2 = pd.DataFrame(res2['params'])\n",
        "        df2['score'] = res2['mean_test_score']\n",
        "        df2['scale'] = 'min_max'\n",
        "\n",
        "\n",
        "        res3 = grid_k[\"grid_RobustScaler\"].cv_results_\n",
        "        df3 = pd.DataFrame(res3['params'])\n",
        "        df3['score'] = res3['mean_test_score']\n",
        "        df3['scale'] = 'robust'\n",
        "\n",
        "        res = [df1, df2, df3]\n",
        "        df = pd.concat(res, ignore_index = True)\n",
        "\n",
        "        M = df['score'].quantile(q = 0.95)\n",
        "        mask = df['score'] > M\n",
        "        return df[mask].sort_values(by = 'score', ascending = False)\n",
        "\n",
        "    def top_las(self):\n",
        "        scaler_type = [StandardScaler, MinMaxScaler, RobustScaler]\n",
        "        grid_k = {}\n",
        "        for s in scaler_type:\n",
        "            pipe = Pipeline([('scale', s()),\n",
        "                            ('classify'  , Lasso(normalize=False, random_state=42))\n",
        "                            ])\n",
        "\n",
        "            hyperparams = {\n",
        "                        'classify__alpha' : np.linspace(0.2, 2, 9)\n",
        "                        }\n",
        "            grid_k[\"grid_\" + s.__name__] = GridSearchCV(pipe, hyperparams, cv=10, scoring='r2').fit(self.X_m, self.y_m)\n",
        "        res1 = grid_k[\"grid_StandardScaler\"].cv_results_\n",
        "        df1 = pd.DataFrame(res1['params'])\n",
        "        df1['score'] = res1['mean_test_score']\n",
        "        df1['scale'] = 'standard'\n",
        "\n",
        "        res2 = grid_k[\"grid_MinMaxScaler\"].cv_results_\n",
        "        df2 = pd.DataFrame(res2['params'])\n",
        "        df2['score'] = res2['mean_test_score']\n",
        "        df2['scale'] = 'min_max'\n",
        "\n",
        "        res3 = grid_k[\"grid_RobustScaler\"].cv_results_\n",
        "        df3 = pd.DataFrame(res3['params'])\n",
        "        df3['score'] = res3['mean_test_score']\n",
        "        df3['scale'] = 'robust'\n",
        "\n",
        "        res = [df1, df2, df3]\n",
        "        df = pd.concat(res, ignore_index = True)\n",
        "        M = df['score'].quantile(q = 0.95)\n",
        "        mask = df['score'] > M\n",
        "        return df[mask].sort_values(by = 'score', ascending = False)\n",
        "\n",
        "    def top_ridge(self):\n",
        "            scaler_type = [StandardScaler, MinMaxScaler, RobustScaler]\n",
        "            grid_k = {}\n",
        "            for s in scaler_type:\n",
        "                pipe = Pipeline([('scale', s()),\n",
        "                                ('classify'  , Ridge(normalize=False, random_state=42))\n",
        "                                ])\n",
        "\n",
        "                hyperparams = {\n",
        "                            'classify__alpha' : np.linspace(0.2, 2, 9)\n",
        "                            }\n",
        "                grid_k[\"grid_\" + s.__name__] = GridSearchCV(pipe, hyperparams, cv=10, scoring='r2').fit(self.X_m, self.y_m)\n",
        "            res1 = grid_k[\"grid_StandardScaler\"].cv_results_\n",
        "            df1 = pd.DataFrame(res1['params'])\n",
        "            df1['score'] = res1['mean_test_score']\n",
        "            df1['scale'] = 'standard'\n",
        "\n",
        "            res2 = grid_k[\"grid_MinMaxScaler\"].cv_results_\n",
        "            df2 = pd.DataFrame(res2['params'])\n",
        "            df2['score'] = res2['mean_test_score']\n",
        "            df2['scale'] = 'min_max'\n",
        "\n",
        "            res3 = grid_k[\"grid_RobustScaler\"].cv_results_\n",
        "            df3 = pd.DataFrame(res3['params'])\n",
        "            df3['score'] = res3['mean_test_score']\n",
        "            df3['scale'] = 'robust'\n",
        "\n",
        "            res = [df1, df2, df3]\n",
        "            df = pd.concat(res, ignore_index = True)\n",
        "            M = df['score'].quantile(q = 0.95)\n",
        "            mask = df['score'] > M\n",
        "            return df[mask].sort_values(by = 'score', ascending = False)\n",
        "\n",
        "    def best_model(self):\n",
        "        knr_model = self.top_knr()\n",
        "        knr_model['model'] = 'knr'\n",
        "        rfr_model = self.top_rfr()\n",
        "        rfr_model['model'] = 'rfr'\n",
        "        lsvr_model = self.top_lsvr()\n",
        "        lsvr_model['model'] = 'lsvr'\n",
        "        las_model = self.top_las()\n",
        "        las_model['model'] = 'lasso'\n",
        "        ridge_model = self.top_ridge()\n",
        "        ridge_model['model'] = 'ridge'\n",
        "\n",
        "        mask1 = knr_model['score'] == knr_model['score'].max()\n",
        "        mask2 = rfr_model['score'] == rfr_model['score'].max()\n",
        "        mask3 = lsvr_model['score'] == lsvr_model['score'].max()\n",
        "        mask4 = las_model['score'] == las_model['score'].max()\n",
        "        mask5 = ridge_model['score'] == ridge_model['score'].max()\n",
        "\n",
        "        full_model = [knr_model[mask1], rfr_model[mask2], lsvr_model[mask3], las_model[mask4], ridge_model[mask5]]\n",
        "        first_highest_score = [knr_model[mask1]['score'].iloc[0], \n",
        "                               rfr_model[mask2]['score'].iloc[0], \n",
        "                               lsvr_model[mask3]['score'].iloc[0], \n",
        "                               las_model[mask4]['score'].iloc[0], \n",
        "                               ridge_model[mask5]['score'].iloc[0]]\n",
        "        return full_model[np.argmax(first_highest_score)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "lA4C1NqO_21g",
        "outputId": "f1c8b772-bd76-4b59-cde5-68e4a2eabd4f"
      },
      "source": [
        "TopModel_Regressor(X_lat_m, y_lat_m).best_model()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>classify__max_depth</th>\n",
              "      <th>score</th>\n",
              "      <th>scale</th>\n",
              "      <th>model</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>0.01178</td>\n",
              "      <td>standard</td>\n",
              "      <td>rfr</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   classify__max_depth    score     scale model\n",
              "0                    2  0.01178  standard   rfr"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "zZGVLAbmlRkc",
        "outputId": "84f97049-fef3-4e7b-8b73-2a8b7dd909aa"
      },
      "source": [
        "TopModel_Regressor(X_yr_m, y_yr_m).best_model()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>classify__max_depth</th>\n",
              "      <th>score</th>\n",
              "      <th>scale</th>\n",
              "      <th>model</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>8</td>\n",
              "      <td>0.033497</td>\n",
              "      <td>robust</td>\n",
              "      <td>rfr</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    classify__max_depth     score   scale model\n",
              "13                    8  0.033497  robust   rfr"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5BHcCD0OFUC"
      },
      "source": [
        "Result Summary: \n",
        "- The best model for predicting the region where the Monarch butterfly was collected was the Random Forest classifier which applied a max depth of 8 decision trees and in which the data was scaled by the robust scaler. \n",
        "\n",
        "- The best model for predicting the country where the Monarch butterfly was collected was the K-Nearest Neighbors classifier which applied the uniform weight function and 20 neighbors and in which the data was scaled by the min-max scaler. \n",
        "\n",
        "- The best model for predicting the latitude in North America where the Monarch butterfly was originally from was the Random Forest regressor which applied a max depth of 2 decision trees and in which the data was scaled by the standard scaler. \n",
        "\n",
        "- The best model for predicting the year when the Monarch butterfly was collected was the Random Forest regressor which applied a max depth of 8 decision trees and in which the data was scaled by the robust scaler. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e92OMaCn2AwU"
      },
      "source": [
        "## Model Performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLsY-4fWS8ML"
      },
      "source": [
        "Using the best models found in the last section, their performance on the holdout sets were evaluated using weighted $F1$-score and the $R^2$ score for the classifiers and regressors, respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "raZcjpZrJFfC",
        "outputId": "e1c4f629-d12e-4e22-f19b-142834434aa9"
      },
      "source": [
        "#Best model for for predicting region based on physical traits\n",
        "pipe = Pipeline([('scale', RobustScaler()),\n",
        "                ('classify', RandomForestClassifier(max_depth=8, random_state=42))\n",
        "                ])\n",
        "y_true = y_reg_h\n",
        "y_pred = pipe.fit(X_reg_m,y_reg_m).predict(X_reg_h) \n",
        "reg_f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "res_r = pd.DataFrame([reg_f1], columns=['Weighted F1'])\n",
        "res_r"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Weighted F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.447145</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Weighted F1\n",
              "0     0.447145"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "GqFtkQgRLEJV",
        "outputId": "3fdd2379-df73-4f6b-acf1-82f729ac0ee5"
      },
      "source": [
        "#Best model for for predicting country based on physical traits\n",
        "pipe = Pipeline([('scale', MinMaxScaler()),\n",
        "                ('classify', KNeighborsClassifier(n_neighbors=20, weights='uniform'))\n",
        "                ])\n",
        "y_true = y_ctry_h\n",
        "y_pred = pipe.fit(X_ctry_m,y_ctry_m).predict(X_ctry_h) \n",
        "ctry_f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "res_c = pd.DataFrame([ctry_f1], columns=['Weighted F1'])\n",
        "res_c"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Weighted F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.223526</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Weighted F1\n",
              "0     0.223526"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "hrD4QEcmMl4z",
        "outputId": "3bc0810a-657f-4976-cb19-6679028f9681"
      },
      "source": [
        "#Best model for predicting the origin of migratory butterflies in North America based on physical traits\n",
        "pipe = Pipeline([('scale', StandardScaler()),\n",
        "                ('classify', RandomForestRegressor(max_depth=2, random_state=42))\n",
        "                ])\n",
        "y_true = y_lat_h\n",
        "y_pred = pipe.fit(X_lat_m,y_lat_m).predict(X_lat_h) \n",
        "lat_R2 = r2_score(y_true, y_pred)\n",
        "res_l = pd.DataFrame([lat_R2], columns=['R2'])\n",
        "res_l"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>R2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.024412</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         R2\n",
              "0  0.024412"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "6ueEqWAtMppo",
        "outputId": "1e548c66-6cb4-4bb7-b49b-2c37f3a98eec"
      },
      "source": [
        "#Best model for predicting year based on physical traits and region\n",
        "pipe = Pipeline([('scale', RobustScaler()),\n",
        "                ('classify', RandomForestRegressor(max_depth=8, random_state=42))\n",
        "                ])\n",
        "y_true = y_yr_h\n",
        "y_pred = pipe.fit(X_yr_m,y_yr_m).predict(X_yr_h) \n",
        "yr_R2 = r2_score(y_true, y_pred)\n",
        "res_y = pd.DataFrame([yr_R2], columns=['R2'])\n",
        "res_y"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>R2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.018742</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         R2\n",
              "0  0.018742"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2vPr-BxfPJV"
      },
      "source": [
        "Overall, the models that applied classifiers performed much better than the models that applied regressors. For the classifiers, the $F1$-score can be interpreted as a weighted average between the preceision and recall with its highest score being 1 and lowest being 0. With a score of around 0.4, the classifier for predicting region performed much better in contrast to the classifier for predicting country. Since the number of neighbors was relatively large for the K-Nearest Neighbors algorithm, a potential issue with the classifer for predicting country would be overfitting. To help resolve this issue, it would be best to test other values for the number of nearest neighbors to better optimize the result. \n",
        "\n",
        "In general, the regressors performed terribly. An $R^2$ score of 0.024 for the random forest regressor for predicting the latititude where the Monarch butterflys came from implies that 2.4% of the variance given in the latitude variable can be explained by the model. The $R^2$ score of 0.019 for the random forest regressor used in predicting the year implied that 1.9% of the variance given in the year variable can be explained by the model. Due to these results, it is possible that a non-linear regressor should be applied instead as a better fit. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lHG-RwJwRVL"
      },
      "source": [
        "## Optimization and Other Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zk9csDffwfZm"
      },
      "source": [
        "To better optimize the results given in section above, the hyperparameters for the best classifier models were adjusted. In addition, the original data was applied to the analysis. That is, the original measurements for the forewings were used instead of the principal components. \n",
        "\n",
        "For the classifer that was used to predict region, the hyperparameter that was adjusted was the max depth of the decision trees used to create the random forests. The maximum depth of each decision tree was set to a whole number from 1 to 14.\n",
        "\n",
        "For the classifer that was used to predict country, the hyperparameter that was adjusted was the number of neighbors for the k-nearest neightbor algorithm.  The number of neighbors was set to a whole number from 1 to 25."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A87u_yIu02dC"
      },
      "source": [
        "X_reg = df_reg[['LLength', 'LWidth', 'LArea', 'LPerimeter', 'RLength', 'RWidth', 'RArea', 'RPerimeter','Sex']]\n",
        "y_reg = df_reg['Region']\n",
        "\n",
        "holdout_frac = 0.20\n",
        "holdout_splitter = StratifiedShuffleSplit(test_size=holdout_frac, random_state=42)\n",
        "model_idx, holdout_idx = next(holdout_splitter.split(X_reg, y_reg))\n",
        "X_reg_m, y_reg_m = X_reg.iloc[model_idx]  , y_reg.iloc[model_idx]\n",
        "X_reg_h, y_reg_h = X_reg.iloc[holdout_idx], y_reg.iloc[holdout_idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTHKbUvF1lF3"
      },
      "source": [
        "X_ctry = df_country[['LLength', 'LWidth', 'LArea', 'LPerimeter', 'RLength', 'RWidth', 'RArea', 'RPerimeter','Sex']]\n",
        "y_ctry = df_country['Country/Archipelago']\n",
        "\n",
        "holdout_frac = 0.20\n",
        "holdout_splitter = StratifiedShuffleSplit(n_splits = 5, test_size=holdout_frac, random_state=42)\n",
        "model_idx, holdout_idx = next(holdout_splitter.split(X_ctry, y_ctry))\n",
        "X_ctry_m, y_ctry_m = X_ctry.iloc[model_idx]  , y_ctry.iloc[model_idx]\n",
        "X_ctry_h, y_ctry_h = X_ctry.iloc[holdout_idx], y_ctry.iloc[holdout_idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtFNzh87wogR"
      },
      "source": [
        "def top_knc(X_m, y_m):\n",
        "    pipe = Pipeline([('scale', MinMaxScaler()),\n",
        "                    ('classify'  , KNeighborsClassifier())\n",
        "                    ])\n",
        "\n",
        "    hyperparams = {\n",
        "                'classify__n_neighbors' : np.arange(1,26),\n",
        "                'classify__weights' : ['uniform', 'distance']\n",
        "                }\n",
        "    grid = GridSearchCV(pipe, hyperparams, cv=10, scoring='f1_weighted').fit(X_m, y_m)\n",
        "    res = grid.cv_results_\n",
        "    df = pd.DataFrame(res['params'])\n",
        "    df['score'] = res['mean_test_score']\n",
        "    df['scale'] = 'min_max'\n",
        "\n",
        "    M = df['score'].quantile(q = 0.9)\n",
        "    mask = df['score'] > M\n",
        "    return df[mask].sort_values(by = 'score', ascending = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qC23MkNx6OR"
      },
      "source": [
        "def top_rfc(X_m, y_m):\n",
        "    pipe = Pipeline([('scale', RobustScaler()),\n",
        "                    ('classify' , RandomForestClassifier(random_state=42))\n",
        "                    ])\n",
        "\n",
        "    hyperparams = {\n",
        "                'classify__max_depth' : np.arange(1, 15)\n",
        "                }\n",
        "    grid = GridSearchCV(pipe, hyperparams, cv=10, scoring='f1_weighted').fit(X_m, y_m)\n",
        "    res = grid.cv_results_\n",
        "    df = pd.DataFrame(res['params'])\n",
        "    df['score'] = res['mean_test_score']\n",
        "    df['scale'] = 'robust'\n",
        "\n",
        "    M = df['score'].quantile(q = 0.9)\n",
        "    mask = df['score'] > M\n",
        "    return df[mask].sort_values(by = 'score', ascending = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "_NucDI5yykxm",
        "outputId": "118fb2c3-0b65-47de-aa28-198d441fb07f"
      },
      "source": [
        "top_rfc(X_reg_m, y_reg_m)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>classify__max_depth</th>\n",
              "      <th>score</th>\n",
              "      <th>scale</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>11</td>\n",
              "      <td>0.449846</td>\n",
              "      <td>robust</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>14</td>\n",
              "      <td>0.449811</td>\n",
              "      <td>robust</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    classify__max_depth     score   scale\n",
              "10                   11  0.449846  robust\n",
              "13                   14  0.449811  robust"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "dhJusfhc0QdX",
        "outputId": "c06c3989-5467-405a-fbea-2251113f9048"
      },
      "source": [
        "#Best optimized model for for predicting region based on physical traits\n",
        "pipe = Pipeline([('scale', RobustScaler()),\n",
        "                ('classify', RandomForestClassifier(max_depth=11, random_state=42))\n",
        "                ])\n",
        "y_true = y_reg_h\n",
        "y_pred = pipe.fit(X_reg_m,y_reg_m).predict(X_reg_h) \n",
        "reg_f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "res_r2 = pd.DataFrame([reg_f1], columns=['New Weighted F1'])\n",
        "pd.concat([res_r, res_r2], axis = 1)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Weighted F1</th>\n",
              "      <th>New Weighted F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.447145</td>\n",
              "      <td>0.444189</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Weighted F1  New Weighted F1\n",
              "0     0.447145         0.444189"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "3kdggJccy3ir",
        "outputId": "110a1fc8-022b-47af-c4c1-5fc78c7f52dc"
      },
      "source": [
        "top_knc(X_ctry_m, y_ctry_m)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>classify__n_neighbors</th>\n",
              "      <th>classify__weights</th>\n",
              "      <th>score</th>\n",
              "      <th>scale</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>11</td>\n",
              "      <td>uniform</td>\n",
              "      <td>0.233033</td>\n",
              "      <td>min_max</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>13</td>\n",
              "      <td>uniform</td>\n",
              "      <td>0.232536</td>\n",
              "      <td>min_max</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>14</td>\n",
              "      <td>uniform</td>\n",
              "      <td>0.231351</td>\n",
              "      <td>min_max</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>10</td>\n",
              "      <td>uniform</td>\n",
              "      <td>0.230556</td>\n",
              "      <td>min_max</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>22</td>\n",
              "      <td>uniform</td>\n",
              "      <td>0.230447</td>\n",
              "      <td>min_max</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    classify__n_neighbors classify__weights     score    scale\n",
              "20                     11           uniform  0.233033  min_max\n",
              "24                     13           uniform  0.232536  min_max\n",
              "26                     14           uniform  0.231351  min_max\n",
              "18                     10           uniform  0.230556  min_max\n",
              "42                     22           uniform  0.230447  min_max"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "zpqtfclP0k6q",
        "outputId": "c51eed21-3091-44eb-930b-1ca77c65c1b4"
      },
      "source": [
        "#Best optimized model for for predicting country based on physical traits\n",
        "pipe = Pipeline([('scale', MinMaxScaler()),\n",
        "                ('classify', KNeighborsClassifier(n_neighbors=11, weights='uniform'))\n",
        "                ])\n",
        "y_true = y_ctry_h\n",
        "y_pred = pipe.fit(X_ctry_m,y_ctry_m).predict(X_ctry_h) \n",
        "ctry_f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "res_c2 = pd.DataFrame([ctry_f1], columns=['New Weighted F1'])\n",
        "pd.concat([res_c, res_c2], axis = 1)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Weighted F1</th>\n",
              "      <th>New Weighted F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.223526</td>\n",
              "      <td>0.238154</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Weighted F1  New Weighted F1\n",
              "0     0.223526         0.238154"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dy6RWv_Z497M"
      },
      "source": [
        "The optimization led to slight decrease in score for the model that predictied region and a slight increase in score for the model that predicted country. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pRuZAkuye6a"
      },
      "source": [
        "In addition to the original models, a neural network model was applied as classifiers for predicting region and country. More specifically, a sequential neural network with one hidden layer was applied. The first layer contained 12 nodes while the hidden layer only contained 5 and for each, the ReLU activation function was applied. To train the neaural network, the Adam optimizer was applied along with the Categorical Cross Entropy loss function and the score used to evaluate the model was the categorical accuracy score. As a note, the choice of the number of layers and nodes in the neural network was arbitrary just to explore how well the network fit to the dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGOca3xy_ftQ"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSnehxaqBtYf"
      },
      "source": [
        "reg_class = df_reg['Region'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZZPagn-Q-wB"
      },
      "source": [
        "df_reg_shuffle = pd.concat([X_reg_m, y_reg_m], axis = 1)\n",
        "df_reg_shuffle['Region'] = pd.Categorical(df_reg_shuffle['Region'])\n",
        "df_reg_shuffle['Region'] = df_reg_shuffle.Region.cat.codes\n",
        "y_reg_m = df_reg_shuffle.pop('Region')\n",
        "X_reg_m = df_reg_shuffle\n",
        "\n",
        "df_reg_shuffle = pd.concat([X_reg_h, y_reg_h], axis = 1)\n",
        "df_reg_shuffle['Region'] = pd.Categorical(df_reg_shuffle['Region'])\n",
        "df_reg_shuffle['Region'] = df_reg_shuffle.Region.cat.codes\n",
        "y_reg_h = df_reg_shuffle.pop('Region')\n",
        "X_reg_h = df_reg_shuffle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_cL9qXVQIG7"
      },
      "source": [
        "data_reg_neural_m = tf.data.Dataset.from_tensor_slices((X_reg_m.values, y_reg_m.values))\n",
        "train_dataset = data_reg_neural_m.shuffle(len(X_reg_m)).batch(1)\n",
        "\n",
        "data_reg_neural_h = tf.data.Dataset.from_tensor_slices((X_reg_h.values, y_reg_h.values))\n",
        "holdout_dataset = data_reg_neural_h.shuffle(len(X_reg_h)).batch(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkHYcJ9QSeqI"
      },
      "source": [
        "def get_compiled_model():\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(12, activation='relu'),\n",
        "    tf.keras.layers.Dense(5, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)\n",
        "  ])\n",
        "\n",
        "  model.compile(optimizer='adam',\n",
        "                loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
        "                metrics=['categorical_accuracy'])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDjJYk_DTK2k",
        "outputId": "5b91bb9c-83f8-4ed4-d306-1fc20717e394"
      },
      "source": [
        "model = get_compiled_model()\n",
        "model.fit(train_dataset, epochs=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "5271/5271 [==============================] - 6s 1ms/step - loss: 3.7898e-07 - categorical_accuracy: 1.0000\n",
            "Epoch 2/3\n",
            "5271/5271 [==============================] - 5s 1ms/step - loss: 3.7800e-07 - categorical_accuracy: 1.0000\n",
            "Epoch 3/3\n",
            "5271/5271 [==============================] - 5s 1ms/step - loss: 3.7822e-07 - categorical_accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc457f7b6d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEtwafEsY5Lu",
        "outputId": "7084920a-6268-4bd1-e8d1-7740c1882813"
      },
      "source": [
        "model.evaluate(holdout_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1318/1318 [==============================] - 1s 748us/step - loss: 3.7843e-07 - categorical_accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3.784307125442865e-07, 1.0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsuBHOn6rsok"
      },
      "source": [
        "df_ctry_shuffle = pd.concat([X_ctry_m, y_ctry_m], axis = 1)\n",
        "df_ctry_shuffle['Country/Archipelago'] = pd.Categorical(df_ctry_shuffle['Country/Archipelago'])\n",
        "df_ctry_shuffle['Country/Archipelago'] = df_ctry_shuffle['Country/Archipelago'].cat.codes\n",
        "y_ctry_m = df_ctry_shuffle.pop('Country/Archipelago')\n",
        "X_ctry_m = df_ctry_shuffle\n",
        "\n",
        "df_ctry_shuffle = pd.concat([X_ctry_h, y_ctry_h], axis = 1)\n",
        "df_ctry_shuffle['Country/Archipelago'] = pd.Categorical(df_ctry_shuffle['Country/Archipelago'])\n",
        "df_ctry_shuffle['Country/Archipelago'] = df_ctry_shuffle['Country/Archipelago'].cat.codes\n",
        "y_ctry_h = df_ctry_shuffle.pop('Country/Archipelago')\n",
        "X_ctry_h = df_ctry_shuffle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-zfohOmuQQz"
      },
      "source": [
        "data_ctry_neural_m = tf.data.Dataset.from_tensor_slices((X_ctry_m.values, y_ctry_m.values))\n",
        "train_dataset = data_ctry_neural_m.shuffle(len(X_ctry_m)).batch(1)\n",
        "\n",
        "data_ctry_neural_h = tf.data.Dataset.from_tensor_slices((X_ctry_h.values, y_ctry_h.values))\n",
        "holdout_dataset = data_ctry_neural_h.shuffle(len(X_ctry_h)).batch(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bgeN0w2vOts",
        "outputId": "1146da3d-71f2-4f8b-ba32-9ac03da2a32c"
      },
      "source": [
        "model = get_compiled_model()\n",
        "model.fit(train_dataset, epochs=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "5233/5233 [==============================] - 6s 1ms/step - loss: 3.1985e-06 - categorical_accuracy: 1.0000\n",
            "Epoch 2/5\n",
            "5233/5233 [==============================] - 5s 1ms/step - loss: 3.1682e-06 - categorical_accuracy: 1.0000\n",
            "Epoch 3/5\n",
            "5233/5233 [==============================] - 5s 1ms/step - loss: 3.1861e-06 - categorical_accuracy: 1.0000\n",
            "Epoch 4/5\n",
            "5233/5233 [==============================] - 5s 1ms/step - loss: 3.1210e-06 - categorical_accuracy: 1.0000\n",
            "Epoch 5/5\n",
            "5233/5233 [==============================] - 5s 1ms/step - loss: 3.2347e-06 - categorical_accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc453803e90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sAHU5GZvSZ8",
        "outputId": "a568f5f9-78bb-44e4-cc33-532a133e7584"
      },
      "source": [
        "model.evaluate(holdout_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1309/1309 [==============================] - 1s 726us/step - loss: 3.1803e-06 - categorical_accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3.1803108413441805e-06, 1.0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqPLAK5U1aPQ"
      },
      "source": [
        "Successfully, the neural networks had nearly 100% accuracy for both models. Due to these results, application of these neural networks to the regression models is recommended. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xg22sj6s2Iw1"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a20ILsLul7-C"
      },
      "source": [
        "In this project, the data collected on the wing measurements of Monarch butterflies from different regions of the world was explored. By applying several different classifier and regression algorithms, several different target variables were predicited from the wing measuremnents. These results are summarized below. \n",
        "\n",
        "- The first model attempted to predict the region a Monarch butterfly was collected based on wing measurements and sex.  This model had the best results using the Random Forest classifier with a max depth of 8 and an $F1$ score of 0.447145.\n",
        "\n",
        "- The second model attempted to predict the country a Monarch butterfly was collected based on wing measurements and sex.  This model had the best results using the K-Nearest Neighbors classifier with 11 neearest neighbors and an $F1$ score of 0.238154. \n",
        "\n",
        "- The third model attempted to predict the latitude a Monarch butterfly was originally from in North America based on wing measurements and sex. The best model for this problem was the Random Forest regressor with a max depth of 2. However, the results that came from this model were relatively poor with an $R^2$ score of 0.024412.\n",
        "\n",
        "- The fourth, and last, model attempted to predict the year the Monarch butterfly was captured based on wing measurements, sex, and region of capture. The best model for this problem was the Random Forest regressor with a max depth of 8. However, the results that came from this model were relatively poor with an $R^2$ score of 0.018742."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axhQqyjs89nh"
      },
      "source": [
        "In addition to the classifiers and regressors used above, neural networks were appied to the first two models. By using a sequential neural network with 12 nodes in the first layes, 5 nodes in the hidden layer, and 1 node in the last layer, the model was able to predict both region and country of capture by almost 100%. \n",
        "\n",
        "If there was more time to pursue this project, the neural network models would be applied to the third and fourth models as well. In addition, other non-linear regressors would be explored such as Support Vector Machine regression with the sigmoid kernel, polynomial regression, and Kernel Ridge regression. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "932uJXD12LgV"
      },
      "source": [
        "## References\n",
        "- Freedman, M. G., Dingle, H., Strauss, S. Y., & Ramírez, S. R. (2020). Two centuries of monarch butterfly collections reveal contrasting effects of range expansion and migration loss on wing traits. *Proceedings of the National Academy of Sciences, 117*(46), 28887-28893.\n"
      ]
    }
  ]
}